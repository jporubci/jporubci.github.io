<!DOCTYPE HTML>
<html>
	<head>
		<title>Modeling Visual Saliency with Eye Tracking</title>
		<meta charset="utf-8"/>
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
		<link rel="stylesheet" href="assets/css/main.css"/>
		<noscript><link rel="stylesheet" href="assets/css/noscript.css"/></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">
				<!-- Header -->
					<header id="header">
						<a class="logo">Jozef Porubcin</a>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Featured Post -->
							<section class="post featured">
								<header class="major">
									<span class="date">June 2023 - Present</span>
									<h2>Modeling Visual Saliency <br>with Eye Tracking</h2>
									<!--<p>Github: <a href="" target="_blank" rel="noopener noreferrer"></a></p>-->
								</header>
								<h3>Introduction</h3>
								<p>The goal of this project is to learn how human saliency can improve deep learning models' ability to generalize beyond their training data. My work is focused around understanding the Pupil Core documentation in order to assist in designing experiments to capture human salience.</p>
								<h3>Background</h3>
								<p>A saliency map represents important parts of an image according to the human visual system. The integration of saliency maps in training deep learning models is believed and proven to improve said models' ability to abstract generalities from data to make more accurate predictions in the face of unseen data. The Pupil Core headset is a headset that records one's pupils and tracks one's gaze in realtime. As such, it is possible to use the Pupil Core headset to create saliency maps.</p>
								<h3>Pupil Core</h3>
								<p>In the beginning, my job was to learn how the Pupil Core headset functions and whether it would be capable of recording and capturing data for long periods of time. I experimented on my own time extensively and documented my experience using the headset and the free software that supports it. Although the learning curve for using the headset is a little steep at first, I was able to get it to work. I created slideshow presentations for the team to show and describe how to use the software for experiments. With time still left during the summer, I was given another task. I was to research gaze-tracking solutions that used the webcam.</p>
								<h3>Webcam-based Solutions</h3>
								<p>Researching webcam-based realtime gaze-tracking solutions that were both viable and free was difficult. Most were not realtime, and those that were typically were not free. I downloaded a handful of GitHub repositories that claimed webcam gaze-tracking in realtime, but not only were they outdated and difficult to set up, but even after spending hours updating the code and figuring out how to make it work with the little documentation provided in the majority of the repositories, the end result was a mess. The best result I ended up with was a program that required its user to remain perfectly still without moving their head forward, backward, side to side, and without rotating their head. The gaze-tracking was still horrible. In the end, I came to the conclusion that any webcam-based gaze-tracking solution would require too much control from the user (i.e. it would require the person to remain far too still for practicality). Furthermore, being open-source and free, most of the applications available were not highly optimized, so they required significant processing power and ran at poor framerates, unsuitable for tracking the eyes' saccades.</p>
								<h3>Future Work</h3>
								<p>Though the summer has passed, I was invited to continue my research this fall. I am currently assisting the team with designing experiments that use the Pupil Core headset. Since I have learned the ins and outs of the headset and software, I am able to provide support to the team. We are currently still designing the experiment and trying to design it in a way that feels natural from the perspective of the participants.</p>
								<h3><br>Acknowledgements</h3>
								<p>Credit to all those that worked on this project at any point, especially <a href="https://www.linkedin.com/in/aileen-dugan-117314175/" target="_blank" rel="noopener noreferrer">Aileen Dugan</a> for her Pupil Core tutorials, <a href="https://www.linkedin.com/in/colton-crum-78a9281b5/" target="_blank" rel="noopener noreferrer">Colton Crum</a> for documenting so many eye tracking solutions and providing detailed Pupil Core setup instructions (among so many other things), and of course Professor Czajka for making this entire opportunity possible.</p>
							</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<section class="split contact">
							<section>
								<h3>Email</h3>
								<p><a href="mailto:jporubci@nd.edu">jporubci@nd.edu</a></p>
							</section>
							<section>
								<h3>Phone</h3>
								<p>+13092306283</p>
							</section>
							<section>
								<h3>Links</h3>
								<ul class="icons alt">
									<li><a href="https://github.com/jporubci" target="_blank" rel="noopener noreferrer" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="https://www.linkedin.com/in/jozef-p/" target="_blank" rel="noopener noreferrer" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
								</ul>
							</section>
						</section>
					</footer>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
