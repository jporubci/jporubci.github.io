<!DOCTYPE HTML>
<html>
	<head>
		<title>Distributed Data Preservation System</title>
		<meta charset="utf-8"/>
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"/>
		<link rel="stylesheet" href="assets/css/main.css"/>
		<noscript><link rel="stylesheet" href="assets/css/noscript.css"/></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">
				<!-- Header -->
					<header id="header">
						<a class="logo">Jozef Porubcin</a>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Featured Post -->
							<section class="post featured">
								<header class="major">
									<span class="date">April - May 2023</span>
									<h2>Distributed Data <br>Preservation System</h2>
									<p>Github: <a href="https://github.com/antithalian/cse40771-group-project" target="_blank" rel="noopener noreferrer">https://github.com/antithalian/cse40771-group-project</a></p>
								</header>
								<h3>Preface</h3>
								<p>Storing information is essential for computers, so data storage solutions will always be required. Since the purpose of storing information is to record things for later use, it follows that effective data solutions must be accurate in their recording of information and reliable in retaining it. Beyond that, it is desirable for a solution to be performant in terms of time and efficient in terms of its use of resources. One of the challenges that comes with storing data on a storage device is that the storage device can fail in its responsibility of retaining the original data. To design a storage system without a single point of failure requires building a distributed storage system with data redundancy where data is replicated across different physical devices, and that is what we set out to do.</p>
								<h3><br>Purpose</h3>
								<p>Our system, sPin, is designed to offload data from clients’ storage devices to sPin’s peers’ storage devices and save the data for a very long time, even if the peers that originally received the data become unavailable for any reason and for any duration. sPin’s goal is to preserve all data that it receives in spite of individual machine failures. The only time that data should be removed from the system is when a deletion request is made for the data. sPin prioritizes the preservation of data over most other issues, particularly data discoverability. Data is stored as files and accessing a file requires knowing its name. Since sPin is designed to be a reliable data <i>storage</i> solution, not a file <i>sharing</i> solution, sPin does not provide clients with a method for extracting the names of files stored on the system.</p>
								<p>Our project takes some inspiration from the InterPlanetary File System protocol (IPFS) in that sPin is also a content-addressed distributed storage system. We adapted IPFS’s concept of pinning in order to help conceptualize our system, and we took some inspiration for our data identification scheme and deletion strategy from a paper describing the strategy used in Celeste which seems to have been a project out of Sun Microsystems with somewhat similar goals to those of our system. In terms of the mechanisms for our system, since we elected not to model our system directly off of Chord or any other system, we had to design something substantially new. The decisions that we made as a result are detailed in the “System Architecture” section below, but, in general, we believe that we came up with a solid set of mechanisms that are appropriate to our desire for an unstructured network of peers.</p>
								<h3><br>System Architecture</h3>
								<p>The three operations that sPin exposes to clients are ADD, GET, and DEL. The ADD operation lets clients request for a copy of a file’s data to be stored in the system. The GET operation allows clients to retrieve a copy of the data of a file in the system. The DEL operation is a way for clients to request a file’s removal from the system.</p>
								<p>There are several large challenges in providing this service to our users. For starters, our peers need some way of finding each other, and clients need a way of finding the system as a whole. Additionally, since we cannot store every single piece of data on every single peer for storage space’s sake, we need a way for peers to communicate with each other about who has what data, as well as for them to retrieve data from their peers and return it to clients. There are also great challenges in ensuring consistency for operations like deletion and attempting to make the system resilient to network partitions. We also need to ensure that our server implementation is robust enough to survive crashes without losing user data or other important information like metadata about the system, since there is a chance that there will be times when only one peer is available with certain pieces of information.</p>
								<p>There are a few ways we can determine whether our system is working correctly. First, we should be able to upload data to a few of our peers, kill off some of them at random including ones that the data is on, and find that the number of peers holding the data converges back to the desired number and that we can still retrieve it. Second, we should be able to permanently delete a file that has been stored on the system, even if the file’s pin goes offline before the deletion request was put in, comes back online, and attempts to redistribute the data after the deletion seemed to have completed from the client’s perspective.</p>
								<p>Under the hood, sPin is an unstructured peer-to-peer network in which each peer shares its table of stored data periodically, allowing the system to converge towards a consistent view of which peers are responsible for which data. sPin’s peers use the <a href="http://catalog.cse.nd.edu:9097/" target="_blank" rel="noopener noreferrer">cclcentral.cse.nd.edu catalog server</a> to make themselves discoverable to other peers for communication purposes. When a client performs an ADD, a universally unique identifier (UUID) is combined with the file’s SHA256 hash to serve as the file’s object ID. The object ID and the file’s data are delivered together to a number of random peers. Recipient peers atomically persist the file to their respective storage devices, naming the file by its object ID, and designate themselves as a “pin” for the file before sending a message of acknowledgement back to the client. A “pin” is simply a peer that is responsible for storing a given file. Each file will have <i>k</i> pins, where <i>k</i> is some ceilinged fraction of <i>n</i>, the total number of peers comprising the system. If <i>k</i> = ceil(<i>n</i> / 3) and <i>n</i> = 5, then <i>k</i> would equal 2, meaning that each file should have two pins. Peers’ object IDs are broadcasted among peers such that sPin is eventually consistent.</p>
								<p>Since peers may go offline, come back online, and new peers may join the system at any time, we needed a solution to naming our servers. Peers automatically configure themselves upon startup. If a peer revives, it will use the name it used previously to register itself in the catalog server. When a peer is born for the first time, it generates a UUID to use as its name, saves it atomically to a file, and then uses it for all future name server registrations. In this way, peers’ identities have a one-to-one association with their respective physical storage devices.</p>
								<p>Along with storing files, peers also keep track of object IDs and their associated files’ pins. Each peer’s pin information is broadcasted periodically along with a timestamp to all known living peers. If a file’s pin has not broadcasted for some time, peers will notice that their information from that pin is stale. When the information becomes too stale, the pin will be assumed dead. When a pin dies, the number of accessible copies for each of its pinned files on the system decreases by one. sPin tries to maintain the number of pins for all files at <i>k</i>, so if a file’s pin count is not equal to <i>k</i>, action will be taken to either designate a new pin for the file, or unassign a pin from a file.</p>
								<p>If the number of pins for a given file is less than <i>k</i>, then the peer whose name is the lowest among the remaining pins for the file is responsible for randomly assigning a new peer as a pin. For example, if <i>n</i> = 8, <i>k</i> = ceil(<i>n</i> / 3) = 3, and one of a file’s pins does not broadcast for enough time, then it will be assumed dead. <i>n</i> will become 7, <i>k</i> will recompute to 3, and the number of pins for the file will be 2. The two remaining pins will both compare their names and determine whose value is lower. The pin with the lower name will randomly choose a new peer to be a pin among the 5 known living peers that are not currently pins for the given file. Once a peer is chosen, the pin will perform an ADD operation, sending the object ID and a copy of the file’s data to the newly chosen peer. The file will be saved atomically and the recipient peer will designate themselves as a pin for the file before sending a message of acknowledgement in response. Once this happens, the information regarding keys and their associated pins will be updated in both of the communicating peers. As always, this information will be broadcasted to the rest of the peers along with the other periodic pin broadcasts from each peer.</p>
								<p>The other possible scenario would be that the number of pins for a given file is greater than <i>k</i>. This could happen if a peer receives a copy of a file and becomes one of its pins, a new peer joins the system, the pin dies, the new peer becomes a new pin, and the original pin comes back online. While we could just say that peers should delete their copy of a file if they come online and the number of pins is already equal to <i>k</i>, it is not possible for peers to differentiate between a peer being dead and a broadcast message simply being delayed. So, even if a pin “dies,” it’s possible that it never really died, and that its broadcast was merely delayed. For this reason, we implement a procedure to handle cases where the number of pins is greater than <i>k</i>.</p>
								<p>If the number of pins for a file is greater than <i>k</i>, the pin whose name is greatest is responsible for randomly choosing a pin (including itself) to drop the file. This way, the pin whose name is lowest does not have to manage adding <i>and</i> dropping pins. Instead, the load for managing pins should be more evenly distributed among all the peers.</p>
								<h3><br>System Performance</h3>
								<p>In order to implement deletion, we rely on object IDs having both a content-based and time-based aspect to them. While researching how to implement deletion in a system like ours that is designed to aggressively spread data if pins believe that there are too few copies, “Deleting Files in the Celeste Peer-to-Peer Storage System” by Badishi et al (link: <a href="https://yld.me/b7c8" target="_blank" rel="noopener noreferrer">https://yld.me/b7c8</a>) proposes an elegant solution. There is a tradeoff in that file identifiers cannot be just a file hash (there needs to be some way to version them) and that deletion requests for files must be recorded. Versioning of our identifiers with a UUID as well as the content hash means that there is no longer a conflict between a delete operation in progress and another client trying to add the same file to the system. Previously, that could have left our system in an inconsistent state where some peers thought that data had been deleted while others thought that it had been deleted and re-added or some other combination of events. Tracking identifiers that have had deletions requested takes up some storage space, but it will allow us to provide a much stronger guarantee that a peer coming back online after having crashed and missed a deletion request will not be able to spread the file back around. Upon receiving a deletion request, a peer stores it in its own deletion list and starts cascading that deletion request around the network. Any peer with the file named as the object ID requested for deletion will delete the file after adding the deletion request to their deletion list. By storing the deletion request, if a peer comes back online later and tries to spread the deleted item, other peers can refuse to take it and forward the deletion back onto the peer trying to spread the item.</p>
								<p>The fact that file’s object IDs are a combination of a UUID and a hash of its contents allows sPin to perform a few optimizations. The UUID allows sPin to effectively version different additions of the same file content in cases like a concurrent addition part of the way through a deletion, while the hash helps us to keep some basis in the content of the file for optimizations like only storing one copy of a given file if a node is pinning multiple versions of it. Overall, this naming scheme is an inexpensive, unique way to identify each and every piece of data ever put into our system. Any client possessing the object ID for a file will be able to locate it on the system, although the lack of an operation to retrieve peers’ object IDs means that files won’t really be discoverable. As our system is focused most on the preservation of data, not its discovery, this is a non-issue.</p>
								<p>We have already described in some detail how our system responds to network failures or individual processes crashing, but we will give it in a little more detail here. Our pinning scheme helps to account for simple cases of network partitioning, although we acknowledge that we have a hard dependency on the name server being reachable. In the case that the name server is unreachable from a peer, the peer will go dormant, waiting until it can connect to the name server. In cases where parts of the peer-to-peer network are partitioned, the pinning and dropping system will allow each partition to start replicating data up to the desired number of copies and keep serving clients who can reach that partition, but possibly not any of the others. In the case of individual processes crashing, our server naming helps us to ensure that every other peer’s view of the network will remain sane when the process comes back up. As each process stores data for the network, it will use the standard “write and move” strategy for atomic writes in order to ensure that all data is atomically written to persistent storage. Similarly, when deletion requests are received and a peer’s table needs to be updated, it performs checkpointing and logging. In case of inconsistencies when a process comes back up, the “source of truth” for what should be in the process’s table will be considered the data in its ‘pins/’ directory (UUID:HASH named files on the filesystem), since any files that a process is a pin for will have been stored atomically and should contain the last good state of the process regarding the data that it was safeguarding. Overall, we believe that these strategies will allow us to be resilient in the face of most network and process failures, so long as nothing catastrophic happens to the entire system all at once (for instance, if an OS kid were to simultaneously fork-bomb every student machine again).</p>
								<p>What follows is a diagram of our system, an overview of many of the operations that we have discussed, and how they will all interact with each other, including the interactions between clients and the peer-to-peer network:</p>
								<div class="image fit"><img src="images/sPin_Diagram.png" alt=""/>
									<div class="credits" style="font-size: 0.5rem; font-style: italic">Link to full PNG: <a href="https://yld.me/raw/gOLI.png" target="_blank" rel="noopener noreferrer">https://yld.me/b7c8>https://yld.me/raw/gOLI.png</a></div>
								</div>
								<p>In order to evaluate our system’s performance, we analyzed it both quantitatively and qualitatively. We wanted to ensure both that our goal of providing reasonable performance to one or more clients uploading, downloading, and deleting files had been met, and we wanted to make sure that our goals of providing durable storage of user data and persisting it across the peer-to-peer network were also met. In order to determine the quantitative performance of our system, we ran a test script that concurrently uploaded, downloaded, and deleted 200 10MB data objects from the system. Tests were done with 1, 3, 5, 7, and 9 peers, which we believed was enough to test the network aspects of our system, and each number of peers was tested with both one client and three concurrent clients. What follows are graphs of our results:</p>
								<div class="image fit"><img src="images/sPIN_ADD_Throughput.png" alt=""/>
									<div class="credits" style="font-size: 0.5rem; font-style: italic"></div>
								</div>
								<div class="image fit"><img src="images/sPIN_ADD_Latency.png" alt=""/>
									<div class="credits" style="font-size: 0.5rem; font-style: italic"></div>
								</div>
								<div class="image fit"><img src="images/sPIN_GET_Throughput.png" alt=""/>
									<div class="credits" style="font-size: 0.5rem; font-style: italic"></div>
								</div>
								<div class="image fit"><img src="images/sPIN_GET_Latency.png" alt=""/>
									<div class="credits" style="font-size: 0.5rem; font-style: italic"></div>
								</div>
								<div class="image fit"><img src="images/sPIN_DEL_Throughput.png" alt=""/>
									<div class="credits" style="font-size: 0.5rem; font-style: italic"></div>
								</div>
								<div class="image fit"><img src="images/sPIN_DEL_Latency.png" alt=""/>
									<div class="credits" style="font-size: 0.5rem; font-style: italic"></div>
								</div>
								<p>Overall, in our quantitative evaluations, we found that our performance was acceptable for the file sizes that we were working with. Especially for the larger peer counts in the three client tests, we were quite happy with an average time to complete each GET and ADD operation of 0.177 seconds and 0.208 seconds respectively. The relatively higher performance of DEL requests makes sense given that much, much less has to be sent over the network for those operations. Instead of uploading or downloading data like the others, all that has to be done is send up the object identifier that the user wishes to delete.</p>
								<p>One thing that we were somewhat surprised by, however, was the fact that our curves did not really look like what we would expect from a distributed system. We had originally expected that adding more peers would help to increase throughput by adding capacity, but, in general, that does not seem to be borne out by our data. We believe that there are two primary reasons for this: inter-peer traffic taking up some of that extra capacity and the fact that the appropriate number of pins, <i>k</i>, increases as the number of peers increases. Basically, instead of getting huge benefits from having more peers for throughput and latency, we trade those benefits for increased resilience of our system against peer failures.</p>
								<p>As we performed our various qualitative tests, we found that we did indeed show good resilience against peer failures, especially as the number of peers increased. We were able to use our CLI client to upload a number of files, kill one or more peers, depending on the number that were present, and then observe the various peers realize that the dead ones were missing. At that point, each remaining peer was able to identify which data objects needed to be replicated and force the others to pin those objects. Similarly, when we brought the dead peers back online, they were able to start showing their pinned objects to the network again, and the network began to reconverge towards <i>k</i> of each data object being present as peers sent drop requests to each other. Finally, when we tested by sending deletes for a pinned object on a dead peer, then bringing the peer back up, we were able to observe the other peers realizing that a deleted object was being advertised again and instruct the peer that was still pinning them to delete the objects, as well.</p>
								<p>Between these quantitative and qualitative observations of our system’s performance in accordance with our goals, we believe that we have achieved our goals of strong resilience to peer failures and good performance from the perspective of our clients. Our design’s self-healing abilities, aiming to reconverge towards a shared view of the network and a shared value of <i>k</i> as peers enter and leave the network, all seem to be working correctly, and we were able to provide good service to our clients, safeguarding their data until they decided they wanted it deleted.</p>
								<h3><br>Conclusion and Future Work</h3>
								<p>We believe that sPin is an effective system for achieving our goals of strong data resilience, good service to our clients, and efficient usage of resources, especially disk, for each peer on the network. As can be seen from the performance data, there were some trade-offs made in order to ensure data durability over performance, but we believe that these were necessary to achieve the durability that we wanted.</p>
								<p>That said, we do believe that, given more time and development resources, a few further additions to the system could be made. For instance, in order to increase performance, it would be helpful to reduce the amount of broadcasting messages around the network that must be done. As we said in the performance evaluation, those broadcasts and coordination between the peers in general limit the latency we can present to clients. A possible solution here would be to use a more structured peer network design and possibly do viral updates instead of broadcasting. There would need to be an evaluation of the effects of these changes on the durability of stored data in the face of peer failures, but it could allow for optimizations like smaller amounts of network traffic and maybe even prioritizing storing data closer to the user, as many Internet-scale eventually consistent systems seem to do. Furthermore, our client RPC calls currently have no good information on which peers store a certain piece of data, resulting in them possibly having to try many peers if there is a partition in the peer network and the first peers it tries cannot retrieve the data on behalf of the client. Including a Bloom filter-like structure in the updates provided to the nameserver could help clients to be smarter about choosing which peers to try to get data from, since they could use the filter as a heuristic to determine which peers might have the data. Finally, using a language other than Python to implement the peer server could have large benefits. Even using Python’s asynchronous capabilities, we experienced a number of race conditions during development and had to pay some performance costs in order to get around them. Another language with stronger concurrency abilities could really increase the performance of the system while maintaining the same durability guarantees.</p>
								<p>Credit to John for co-authoring the source of this article.</p>
							</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<section class="split contact">
							<section>
								<h3>Email</h3>
								<p><a href="mailto:jporubci@nd.edu">jporubci@nd.edu</a></p>
							</section>
							<section>
								<h3>Phone</h3>
								<p>+13092306283</p>
							</section>
							<section>
								<h3>Links</h3>
								<ul class="icons alt">
									<li><a href="https://github.com/jporubci" target="_blank" rel="noopener noreferrer" class="icon brands alt fa-github"><span class="label">GitHub</span></a></li>
									<li><a href="https://www.linkedin.com/in/jozef-p/" target="_blank" rel="noopener noreferrer" class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a></li>
								</ul>
							</section>
						</section>
					</footer>
			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
